#!/usr/bin/env python3
"""
–°–∏—Å—Ç–µ–º–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–æ–≤ –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π
–Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
"""

import json
from dataclasses import dataclass
from typing import Dict, List, Tuple
from datetime import datetime

@dataclass
class ModelCapabilities:
    math: float
    code: float
    text: float
    analysis: float
    creative: float
    explanation: float
    planning: float
    research: float
    optimization: float

@dataclass
class TechnicalSpecs:
    avg_response_time: float
    cost_per_1k_tokens: float
    reliability_score: float
    context_window: int
    max_output_tokens: int

# –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π (–∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤)
MODEL_CAPABILITIES = {
    "GPT-4": ModelCapabilities(
        math=56.8,      # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –≤ 0-100 —à–∫–∞–ª—É
        code=50.0, 
        text=61.8,
        analysis=47.7,
        creative=85.3,
        explanation=53.5,
        planning=61.8,
        research=50.0,
        optimization=61.7
    ),
    "Claude-3.5-Sonnet": ModelCapabilities(
        math=52.3,
        code=50.0, 
        text=85.3,
        analysis=46.9,
        creative=61.8,
        explanation=60.5,
        planning=65.7,
        research=50.0,
        optimization=85.2
    ),
    "GPT-3.5-Turbo": ModelCapabilities(
        math=53.0,
        code=50.0, 
        text=65.7,
        analysis=43.0,
        creative=65.7,
        explanation=54.7,
        planning=85.4,
        research=50.0,
        optimization=65.6
    )
}

# –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –º–æ–¥–µ–ª–µ–π
TECHNICAL_SPECS = {
    "GPT-4": TechnicalSpecs(
        avg_response_time=2.5,
        cost_per_1k_tokens=0.03,
        reliability_score=95.0,
        context_window=32000,
        max_output_tokens=4000
    ),
    "Claude-3.5-Sonnet": TechnicalSpecs(
        avg_response_time=2.2,
        cost_per_1k_tokens=0.025,
        reliability_score=92.0,
        context_window=200000,
        max_output_tokens=4096
    ),
    "GPT-3.5-Turbo": TechnicalSpecs(
        avg_response_time=1.8,
        cost_per_1k_tokens=0.002,
        reliability_score=88.0,
        context_window=16000,
        max_output_tokens=2048
    )
}

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –∑–∞–¥–∞—á –∏ –∏—Ö –æ–ø–∏—Å–∞–Ω–∏–π
TASK_TYPES = {
    'math': '–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏',
    'code': '–ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ',
    'text': '–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞',
    'analysis': '–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö',
    'creative': '–¢–≤–æ—Ä—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏',
    'explanation': '–û–±—ä—è—Å–Ω–µ–Ω–∏—è',
    'planning': '–ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ',
    'research': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è',
    'optimization': '–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è'
}

class BenchmarkReportGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ—Ç—á–µ—Ç–æ–≤ –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self):
        self.models = MODEL_CAPABILITIES
        self.specs = TECHNICAL_SPECS
        
    def get_task_rankings(self, task_type: str) -> List[Tuple[str, float]]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–π—Ç–∏–Ω–≥ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–∏–ø–∞ –∑–∞–¥–∞—á"""
        if task_type not in TASK_TYPES:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –∑–∞–¥–∞—á: {task_type}")
            
        rankings = []
        for model_name, capabilities in self.models.items():
            score = getattr(capabilities, task_type)
            rankings.append((model_name, score))
            
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        rankings.sort(key=lambda x: x[1], reverse=True)
        return rankings
    
    def get_overall_rankings(self) -> List[Tuple[str, float]]:
        """–ü–æ–ª—É—á–∏—Ç—å –æ–±—â–∏–π —Ä–µ–π—Ç–∏–Ω–≥ –º–æ–¥–µ–ª–µ–π"""
        overall_scores = {}
        
        for model_name, capabilities in self.models.items():
            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª –ø–æ –≤—Å–µ–º –∑–∞–¥–∞—á–∞–º
            scores = [
                capabilities.math, capabilities.code, capabilities.text,
                capabilities.analysis, capabilities.creative, capabilities.explanation,
                capabilities.planning, capabilities.research, capabilities.optimization
            ]
            overall_scores[model_name] = sum(scores) / len(scores)
            
        rankings = list(overall_scores.items())
        rankings.sort(key=lambda x: x[1], reverse=True)
        return rankings
    
    def get_best_model_for_task(self, task_type: str) -> Tuple[str, float]:
        """–ù–∞–π—Ç–∏ –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–∏–ø–∞ –∑–∞–¥–∞—á"""
        rankings = self.get_task_rankings(task_type)
        return rankings[0] if rankings else ("–ù–µ –Ω–∞–π–¥–µ–Ω–æ", 0.0)
    
    def get_model_comparison_matrix(self) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å –º–∞—Ç—Ä–∏—Ü—É —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –ø–æ –≤—Å–µ–º –∑–∞–¥–∞—á–∞–º"""
        matrix = {}
        
        for task_type in TASK_TYPES.keys():
            matrix[task_type] = {}
            for model_name, capabilities in self.models.items():
                score = getattr(capabilities, task_type)
                matrix[task_type][model_name] = score
                
        return matrix
    
    def calculate_efficiency_score(self, model_name: str) -> float:
        """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å/—Å—Ç–æ–∏–º–æ—Å—Ç—å)"""
        if model_name not in self.models:
            return 0.0
            
        # –°—Ä–µ–¥–Ω—è—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        capabilities = self.models[model_name]
        avg_performance = sum([
            capabilities.math, capabilities.code, capabilities.text,
            capabilities.analysis, capabilities.creative, capabilities.explanation,
            capabilities.planning, capabilities.research, capabilities.optimization
        ]) / 9
        
        # –°—Ç–æ–∏–º–æ—Å—Ç—å
        cost = self.specs[model_name].cost_per_1k_tokens
        
        # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å = –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å / —Å—Ç–æ–∏–º–æ—Å—Ç—å
        # –£–º–Ω–æ–∂–∞–µ–º –Ω–∞ 1000 –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ —á—Ç–µ–Ω–∏—è
        efficiency = (avg_performance / (cost * 1000)) if cost > 0 else 0
        return efficiency
    
    def generate_detailed_report(self) -> Dict:
        """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –≤—Å–µ–º –º–æ–¥–µ–ª—è–º"""
        report = {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "models_evaluated": list(self.models.keys()),
                "task_types": list(TASK_TYPES.keys())
            },
            "overall_rankings": [],
            "task_specific_rankings": {},
            "best_models_by_task": {},
            "technical_comparison": {},
            "efficiency_rankings": [],
            "performance_matrix": self.get_model_comparison_matrix()
        }
        
        # –û–±—â–∏–π —Ä–µ–π—Ç–∏–Ω–≥
        overall = self.get_overall_rankings()
        for rank, (model, score) in enumerate(overall, 1):
            report["overall_rankings"].append({
                "rank": rank,
                "model": model,
                "average_score": round(score, 2)
            })
        
        # –†–µ–π—Ç–∏–Ω–≥–∏ –ø–æ —Ç–∏–ø–∞–º –∑–∞–¥–∞—á
        for task_type, task_desc in TASK_TYPES.items():
            rankings = self.get_task_rankings(task_type)
            report["task_specific_rankings"][task_type] = {
                "description": task_desc,
                "rankings": [
                    {"rank": rank, "model": model, "score": round(score, 2)}
                    for rank, (model, score) in enumerate(rankings, 1)
                ]
            }
            
            # –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ –∑–∞–¥–∞—á
            best_model, best_score = self.get_best_model_for_task(task_type)
            report["best_models_by_task"][task_type] = {
                "task_description": task_desc,
                "best_model": best_model,
                "score": round(best_score, 2)
            }
        
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        for model_name, specs in self.specs.items():
            report["technical_comparison"][model_name] = {
                "response_time": specs.avg_response_time,
                "cost_per_1k_tokens": specs.cost_per_1k_tokens,
                "reliability": specs.reliability_score,
                "context_window": specs.context_window,
                "max_output": specs.max_output_tokens
            }
        
        # –†–µ–π—Ç–∏–Ω–≥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        efficiency_scores = []
        for model_name in self.models.keys():
            efficiency = self.calculate_efficiency_score(model_name)
            efficiency_scores.append((model_name, efficiency))
        
        efficiency_scores.sort(key=lambda x: x[1], reverse=True)
        for rank, (model, score) in enumerate(efficiency_scores, 1):
            report["efficiency_rankings"].append({
                "rank": rank,
                "model": model,
                "efficiency_score": round(score, 2)
            })
        
        return report
    
    def print_summary_report(self):
        """–ù–∞–ø–µ—á–∞—Ç–∞—Ç—å –∫—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç –≤ –∫–æ–Ω—Å–æ–ª–∏"""
        print("\n" + "="*80)
        print("üìä –û–¢–ß–ï–¢ –ü–û –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò –ú–û–î–ï–õ–ï–ô (–Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤)")
        print("="*80)
        
        # –û–±—â–∏–π —Ä–µ–π—Ç–∏–Ω–≥
        print("\nüèÜ –û–ë–©–ò–ô –†–ï–ô–¢–ò–ù–ì –ú–û–î–ï–õ–ï–ô:")
        overall = self.get_overall_rankings()
        for rank, (model, score) in enumerate(overall, 1):
            print(f"  {rank}. {model:<20} - {score:.1f} –±–∞–ª–ª–æ–≤")
        
        # –õ—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ —Ç–∏–ø–∞–º –∑–∞–¥–∞—á
        print("\nüéØ –õ–£–ß–®–ò–ï –ú–û–î–ï–õ–ò –ü–û –¢–ò–ü–ê–ú –ó–ê–î–ê–ß:")
        for task_type, task_desc in TASK_TYPES.items():
            best_model, best_score = self.get_best_model_for_task(task_type)
            print(f"  {task_desc:<20} - {best_model} ({best_score:.1f})")
        
        # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
        print("\nüí∞ –†–ï–ô–¢–ò–ù–ì –≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–ò (–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å/—Å—Ç–æ–∏–º–æ—Å—Ç—å):")
        for model_name in self.models.keys():
            efficiency = self.calculate_efficiency_score(model_name)
            print(f"  {model_name:<20} - {efficiency:.1f}")
        
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        print("\n‚öôÔ∏è  –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö–ò:")
        for model_name, specs in self.specs.items():
            print(f"  {model_name}:")
            print(f"    –í—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞: {specs.avg_response_time}—Å")
            print(f"    –°—Ç–æ–∏–º–æ—Å—Ç—å: ${specs.cost_per_1k_tokens:.3f}/1k —Ç–æ–∫–µ–Ω–æ–≤")
            print(f"    –ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å: {specs.reliability_score}%")
            print(f"    –ö–æ–Ω—Ç–µ–∫—Å—Ç: {specs.context_window:,} —Ç–æ–∫–µ–Ω–æ–≤")
        
        print("\n" + "="*80)

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–æ–≤"""
    generator = BenchmarkReportGenerator()
    
    # –ö—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç –≤ –∫–æ–Ω—Å–æ–ª–∏
    generator.print_summary_report()
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –≤ JSON
    detailed_report = generator.generate_detailed_report()
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
    with open('benchmark_performance_report.json', 'w', encoding='utf-8') as f:
        json.dump(detailed_report, f, ensure_ascii=False, indent=2)
    
    print(f"\n‚úÖ –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: benchmark_performance_report.json")
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞
    print("\nüìà –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –ê–ù–ê–õ–ò–¢–ò–ö–ê:")
    
    # –ü–æ–∫–∞–∑–∞—Ç—å –∑–∞–¥–∞—á–∏, –≥–¥–µ –∫–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å –ª—É—á—à–∞—è
    model_strengths = {}
    for task_type in TASK_TYPES.keys():
        best_model, _ = generator.get_best_model_for_task(task_type)
        if best_model not in model_strengths:
            model_strengths[best_model] = []
        model_strengths[best_model].append(TASK_TYPES[task_type])
    
    for model, strengths in model_strengths.items():
        print(f"  {model} - –ª—É—á—à–∞—è –≤: {', '.join(strengths)}")

if __name__ == "__main__":
    main()
