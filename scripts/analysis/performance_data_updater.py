"""
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏ –æ–±–Ω–æ–≤–ª—è—Ç–µ–ª—å –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ LLM –º–æ–¥–µ–ª–µ–π
–Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.
"""

import json
import numpy as np
from typing import Dict, List, Any, Tuple
from collections import defaultdict
from dataclasses import dataclass
from pathlib import Path
import statistics

@dataclass
class TaskPerformanceMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–∏–ø–∞ –∑–∞–¥–∞—á"""
    success_rate: float
    avg_duration: float
    avg_tokens: int
    avg_cost: float
    quality_score: float  # –°—É–±—ä–µ–∫—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    response_completeness: float  # –ü–æ–ª–Ω–æ—Ç–∞ –æ—Ç–≤–µ—Ç–∞
    
class PerformanceDataUpdater:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤"""
    
    def __init__(self, test_results_path: str = "test_results/real_llm_test_results.json"):
        self.test_results_path = test_results_path
        self.task_type_mapping = {
            'math': 'math',
            'code': 'code', 
            'text': 'text',
            'analysis': 'analysis',
            'creative': 'creative',
            'explanation': 'explanation',
            'planning': 'planning',
            'research': 'research',
            'optimization': 'optimization'
        }
        
    def load_test_results(self) -> List[Dict[str, Any]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–æ–≤ –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        try:
            with open(self.test_results_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–¥–∞—á–∏ –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
            tasks = []
            if 'task_executions' in data:
                tasks = data['task_executions']
            elif isinstance(data, list):
                tasks = data
            else:
                # –ò—â–µ–º –∑–∞–¥–∞—á–∏ –≤ –¥—Ä—É–≥–∏—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä–∞—Ö
                for key, value in data.items():
                    if isinstance(value, list) and len(value) > 0:
                        if 'task_id' in value[0] or 'execution_result' in value[0]:
                            tasks = value
                            break
                            
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(tasks)} –∑–∞–¥–∞—á –∏–∑ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return tasks
            
        except FileNotFoundError:
            print(f"–§–∞–π–ª {self.test_results_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return []
        except json.JSONDecodeError:
            print(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è JSON —Ñ–∞–π–ª–∞ {self.test_results_path}")
            return []
            
    def extract_executor_performance(self, tasks: List[Dict[str, Any]]) -> Dict[str, Dict[str, List[TaskPerformanceMetrics]]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è–º –∏ —Ç–∏–ø–∞–º –∑–∞–¥–∞—á"""
        
        executor_performance = defaultdict(lambda: defaultdict(list))
        
        for task in tasks:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∑–∞–¥–∞—á–∏
            execution_result = task.get('execution_result', task)
            
            executor_id = execution_result.get('executor_id', 'unknown')
            task_type = execution_result.get('task_type', 'unknown')
            status = execution_result.get('status', 'unknown')
            duration = execution_result.get('duration', 0)
            tokens = execution_result.get('tokens', 0)
            cost = execution_result.get('cost', 0)
            result_text = execution_result.get('result', '')
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–∏–ø –∑–∞–¥–∞—á–∏
            normalized_task_type = self.task_type_mapping.get(task_type, task_type)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
            success_rate = 1.0 if status == 'success' else 0.0
            quality_score = self._evaluate_response_quality(result_text, normalized_task_type)
            completeness = self._evaluate_response_completeness(result_text)
            
            metrics = TaskPerformanceMetrics(
                success_rate=success_rate,
                avg_duration=duration,
                avg_tokens=tokens,
                avg_cost=cost,
                quality_score=quality_score,
                response_completeness=completeness
            )
            
            executor_performance[executor_id][normalized_task_type].append(metrics)
            
        return dict(executor_performance)
    
    def _evaluate_response_quality(self, response: str, task_type: str) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –∏ —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏"""
        if not response or len(response.strip()) == 0:
            return 0.0
            
        # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã –æ—Ç–≤–µ—Ç–∞
        length_score = min(len(response) / 500, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ 1.0
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏
        type_specific_score = 0.5  # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
        
        if task_type == 'code':
            # –î–ª—è –∫–æ–¥–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            code_indicators = ['def ', 'class ', 'import ', 'return ', '{}', '[]', '()']
            found_indicators = sum(1 for indicator in code_indicators if indicator in response)
            type_specific_score = min(found_indicators / len(code_indicators), 1.0)
            
        elif task_type == 'math':
            # –î–ª—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —á–∏—Å–µ–ª –∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
            math_indicators = ['=', '+', '-', '*', '/', '(', ')', 'x', 'y']
            found_indicators = sum(1 for indicator in math_indicators if indicator in response)
            type_specific_score = min(found_indicators / 5, 1.0)
            
        elif task_type == 'analysis':
            # –î–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞
            structure_indicators = ['1.', '2.', '‚Ä¢', '-', ':', '\n\n']
            found_indicators = sum(1 for indicator in structure_indicators if indicator in response)
            type_specific_score = min(found_indicators / 3, 1.0)
            
        # –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞–∫ —Å—Ä–µ–¥–Ω–µ–µ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ
        return (length_score * 0.3 + type_specific_score * 0.7)
    
    def _evaluate_response_completeness(self, response: str) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø–æ–ª–Ω–æ—Ç—É –æ—Ç–≤–µ—Ç–∞"""
        if not response:
            return 0.0
            
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        words = len(response.split())
        sentences = response.count('.') + response.count('!') + response.count('?')
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        word_score = min(words / 100, 1.0)  # 100+ —Å–ª–æ–≤ = –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        sentence_score = min(sentences / 5, 1.0)  # 5+ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π = –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        
        return (word_score + sentence_score) / 2
    
    def aggregate_metrics(self, executor_performance: Dict[str, Dict[str, List[TaskPerformanceMetrics]]]) -> Dict[str, Dict[str, TaskPerformanceMetrics]]:
        """–ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è–º –∏ —Ç–∏–ø–∞–º –∑–∞–¥–∞—á"""
        
        aggregated = {}
        
        for executor_id, task_types in executor_performance.items():
            aggregated[executor_id] = {}
            
            for task_type, metrics_list in task_types.items():
                if not metrics_list:
                    continue
                    
                # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
                avg_success_rate = statistics.mean([m.success_rate for m in metrics_list])
                avg_duration = statistics.mean([m.avg_duration for m in metrics_list])
                avg_tokens = int(statistics.mean([m.avg_tokens for m in metrics_list]))
                avg_cost = statistics.mean([m.avg_cost for m in metrics_list])
                avg_quality = statistics.mean([m.quality_score for m in metrics_list])
                avg_completeness = statistics.mean([m.response_completeness for m in metrics_list])
                
                aggregated_metrics = TaskPerformanceMetrics(
                    success_rate=avg_success_rate,
                    avg_duration=avg_duration,
                    avg_tokens=avg_tokens,
                    avg_cost=avg_cost,
                    quality_score=avg_quality,
                    response_completeness=avg_completeness
                )
                
                aggregated[executor_id][task_type] = aggregated_metrics
                
        return aggregated
    
    def convert_to_model_scores(self, aggregated_metrics: Dict[str, Dict[str, TaskPerformanceMetrics]]) -> Dict[str, Dict[str, float]]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π (0-1)"""
        
        model_scores = {}
        
        # –ú–∞–ø–ø–∏–Ω–≥ –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª–µ–π –Ω–∞ –º–æ–¥–µ–ª–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)
        executor_to_model = {
            'real_executor_0': 'GPT-4',
            'real_executor_1': 'Claude-3.5-Sonnet', 
            'real_executor_2': 'GPT-3.5-Turbo',
            'real_executor_3': 'Gemini-Pro',
            'real_executor_4': 'Llama-3-70B'
        }
        
        for executor_id, task_metrics in aggregated_metrics.items():
            model_name = executor_to_model.get(executor_id, f"Model_{executor_id}")
            model_scores[model_name] = {}
            
            for task_type, metrics in task_metrics.items():
                # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —É—á–∏—Ç—ã–≤–∞—é—â–∞—è –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–∫—Ç–æ—Ä–æ–≤
                performance_score = (
                    metrics.success_rate * 0.3 +  # 30% - —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
                    metrics.quality_score * 0.4 +  # 40% - –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–∞
                    metrics.response_completeness * 0.2 +  # 20% - –ø–æ–ª–Ω–æ—Ç–∞ –æ—Ç–≤–µ—Ç–∞
                    (1 - min(metrics.avg_duration / 10, 1)) * 0.1  # 10% - —Å–∫–æ—Ä–æ—Å—Ç—å (–∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è)
                )
                
                model_scores[model_name][task_type] = round(performance_score, 3)
                
        return model_scores
    
    def generate_updated_evaluator_code(self, model_scores: Dict[str, Dict[str, float]]) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –∫–æ–¥ –¥–ª—è model_performance_evaluator.py"""
        
        # –ß–∏—Ç–∞–µ–º —Ç–µ–∫—É—â–∏–π –∫–æ–¥ evaluator'–∞
        try:
            with open('model_performance_evaluator.py', 'r', encoding='utf-8') as f:
                current_code = f.read()
        except FileNotFoundError:
            print("–§–∞–π–ª model_performance_evaluator.py –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return ""
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –æ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è—Ö –º–æ–¥–µ–ª–µ–π
        capabilities_data = []
        
        for model_name, scores in model_scores.items():
            # –ó–∞–ø–æ–ª–Ω—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ —Ç–∏–ø—ã –∑–∞–¥–∞—á —Å—Ä–µ–¥–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
            all_task_types = ['math', 'code', 'text', 'analysis', 'creative', 'explanation', 'planning', 'research', 'optimization']
            
            for task_type in all_task_types:
                if task_type not in scores:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ –≤—Å–µ–º –º–æ–¥–µ–ª—è–º –¥–ª—è —ç—Ç–æ–≥–æ —Ç–∏–ø–∞ –∑–∞–¥–∞—á
                    avg_score = 0.5  # –î–µ—Ñ–æ–ª—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                    if model_scores:
                        type_scores = [m.get(task_type, 0.5) for m in model_scores.values()]
                        if type_scores:
                            avg_score = statistics.mean([s for s in type_scores if s > 0])
                    scores[task_type] = avg_score
            
            capability = f'''    "{model_name}": ModelCapabilities(
        math={scores.get('math', 0.5):.3f},
        code={scores.get('code', 0.5):.3f}, 
        text={scores.get('text', 0.5):.3f},
        analysis={scores.get('analysis', 0.5):.3f},
        creative={scores.get('creative', 0.5):.3f},
        explanation={scores.get('explanation', 0.5):.3f},
        planning={scores.get('planning', 0.5):.3f},
        research={scores.get('research', 0.5):.3f},
        optimization={scores.get('optimization', 0.5):.3f},
        technical_specs=TechnicalSpecs(
            avg_response_time=2.0,  # –û–±–Ω–æ–≤–∏—Ç—Å—è –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            cost_per_1k_tokens=0.02,
            reliability_score=0.95,
            context_window=32000,
            max_output_tokens=4000
        )
    )'''
            capabilities_data.append(capability)
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –±–ª–æ–∫ MODEL_CAPABILITIES
        new_capabilities_block = "MODEL_CAPABILITIES = {\n" + ",\n".join(capabilities_data) + "\n}"
        
        return new_capabilities_block
    
    def update_model_evaluator(self) -> bool:
        """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è evaluator'–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        
        print("üîÑ –ù–∞—á–∏–Ω–∞—é –∞–Ω–∞–ª–∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–æ–≤
        tasks = self.load_test_results()
        if not tasks:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ")
            return False
            
        # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        executor_performance = self.extract_executor_performance(tasks)
        print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(executor_performance)} –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª–µ–π")
        
        # 3. –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏
        aggregated = self.aggregate_metrics(executor_performance)
        
        # 4. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π
        model_scores = self.convert_to_model_scores(aggregated)
        print(f"ü§ñ –†–∞—Å—Å—á–∏—Ç–∞–Ω—ã –æ—Ü–µ–Ω–∫–∏ –¥–ª—è {len(model_scores)} –º–æ–¥–µ–ª–µ–π")
        
        # 5. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –∫–æ–¥
        updated_code = self.generate_updated_evaluator_code(model_scores)
        
        if updated_code:
            # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª —Å –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            with open('updated_model_capabilities.py', 'w', encoding='utf-8') as f:
                f.write(f"# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π\n")
                f.write(f"# –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n\n")
                f.write(updated_code)
                
            print("‚úÖ –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª updated_model_capabilities.py —Å –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç
            report = {
                "updated_models": list(model_scores.keys()),
                "model_scores": model_scores,
                "aggregated_metrics": {
                    executor: {
                        task_type: {
                            "success_rate": metrics.success_rate,
                            "avg_duration": metrics.avg_duration,
                            "avg_tokens": metrics.avg_tokens,
                            "avg_cost": metrics.avg_cost,
                            "quality_score": metrics.quality_score,
                            "response_completeness": metrics.response_completeness
                        }
                        for task_type, metrics in task_metrics.items()
                    }
                    for executor, task_metrics in aggregated.items()
                },
                "total_tasks_analyzed": len(tasks),
                "update_timestamp": "2025-01-27"
            }
            
            with open('performance_update_report.json', 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
                
            print("üìã –°–æ–∑–¥–∞–Ω –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç performance_update_report.json")
            return True
        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –∫–æ–¥")
            return False

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è"""
    
    updater = PerformanceDataUpdater()
    
    print("üöÄ –ó–∞–ø—É—Å–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π...")
    print("="*70)
    
    success = updater.update_model_evaluator()
    
    if success:
        print("\n" + "="*70)
        print("‚úÖ –û–ë–ù–û–í–õ–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
        print("üìÅ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª—ã:")
        print("   ‚Ä¢ updated_model_capabilities.py - –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π")
        print("   ‚Ä¢ performance_update_report.json - –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç –æ–± –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏")
        print("\nüí° –í—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —Å–≤–æ–µ–π —Å–∏—Å—Ç–µ–º–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏.")
    else:
        print("\n‚ùå –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –ø–æ–ø—ã—Ç–∫—É.")

if __name__ == "__main__":
    main()
